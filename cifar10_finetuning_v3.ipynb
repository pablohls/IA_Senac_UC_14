{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install transformers datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "# For dislaying images\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "# Loading dataset\n",
    "from datasets import load_dataset\n",
    "# Transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Matrix operations\n",
    "import numpy as np\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds, testds = load_dataset(\"cifar10\", split=[\"train[:5000]\",\"test[:1000]\"])\n",
    "splits = trainds.train_test_split(test_size=0.1)\n",
    "trainds = splits['train']\n",
    "valds = splits['test']\n",
    "trainds, valds, testds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = dict((k,v) for k,v in enumerate(trainds.features['label'].names))\n",
    "stoi = dict((v,k) for k,v in enumerate(trainds.features['label'].names))\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "img, lab = trainds[index]['img'], itos[trainds[index]['label']]\n",
    "print(lab)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ae09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name) \n",
    "\n",
    "mu, sigma = processor.image_mean, processor.image_std #get default mu,sigma\n",
    "size = processor.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b09cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize(mean=mu, std=sigma) #normalize image pixels range to [-1,1]\n",
    "\n",
    "# resize 3x32x32 to 3x224x224 -> convert to Pytorch tensor -> normalize\n",
    "_transf = Compose([\n",
    "    Resize(size['height']),\n",
    "    ToTensor(),\n",
    "    norm\n",
    "]) \n",
    "\n",
    "# apply transforms to PIL Image and store it to 'pixels' key\n",
    "def transf(arg):\n",
    "    arg['pixels'] = [_transf(image.convert('RGB')) for image in arg['img']]\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99372d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds.set_transform(transf)\n",
    "valds.set_transform(transf)\n",
    "testds.set_transform(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "ex = trainds[idx]['pixels']\n",
    "ex = (ex+1)/2 #imshow requires image pixels to be in the range [0,1]\n",
    "exi = ToPILImage()(ex)\n",
    "plt.imshow(exi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a61505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a51af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=10, ignore_mismatched_sizes=True, id2label=itos, label2id=stoi)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-cifar-10\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy =\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ee70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixels = torch.stack([example[\"pixels\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08efcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args, \n",
    "    train_dataset=trainds,\n",
    "    eval_dataset=valds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d07ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(testds)\n",
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[np.argmax(outputs.predictions[0])], itos[outputs.label_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = trainds.features['label'].names\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(xticks_rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
