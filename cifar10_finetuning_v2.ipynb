{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install transformers datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: IMPORTS - VERS√ÉO PARA DATASET CUSTOMIZADO\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: Adicionamos pandas, os, e Dataset do PyTorch\n",
    "# RAZ√ÉO: Necess√°rios para carregar dados de CSV e criar dataset customizado\n",
    "# COMPARA√á√ÉO: Antes us√°vamos datasets.load_dataset (HuggingFace). Agora:\n",
    "#   - pandas: para ler/manipular CSV\n",
    "#   - os: para navegar sistema de arquivos\n",
    "#   - Dataset: base para criar CustomImageDataset\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "# For displaying images\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "# Loading dataset - ALTERADO para CustomImageDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# Transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Matrix operations\n",
    "import numpy as np\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia e compara√ß√£o\n",
    "# ============================================================================\n",
    "# # Loading dataset - ORIGINAL\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: CARREGAR E PREPARAR DATASET - VERS√ÉO CUSTOMIZADA\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO PRINCIPAL: Substituir load_dataset por CustomImageDataset\n",
    "# RAZ√ÉO: Em cen√°rios reais, os dados est√£o em CSV + pastas de imagens\n",
    "# ESTRUTURA DE DADOS:\n",
    "#   /workspace/data/train.csv ‚Üí file_id,class (14035 samples com labels)\n",
    "#   /workspace/data/train/*.jpg ‚Üí imagens\n",
    "#   /workspace/data/test.csv ‚Üí file_id (4945 samples SEM labels - cen√°rio real)\n",
    "#   /workspace/data/test/*.jpg ‚Üí imagens para predi√ß√£o\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE CUSTOMIZADA: CustomImageDataset\n",
    "# ============================================================================\n",
    "# RAZ√ÉO: Permite flexibilidade total no carregamento de dados customizados\n",
    "# VANTAGENS vs Hugging Face datasets:\n",
    "#   1. Simples de entender e modificar\n",
    "#   2. Suporta dados sem labels (test set real)\n",
    "#   3. F√°cil adicionar novas imagens (s√≥ alterar CSV)\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset customizado para carregar imagens de pastas com metadata em CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Caminho para arquivo CSV com colunas [file_id, class]\n",
    "        img_dir (str): Diret√≥rio contendo as imagens\n",
    "        transform (callable, optional): Transforma√ß√µes a aplicar nas imagens\n",
    "        has_labels (bool): Se True, CSV tem coluna 'class'. Se False, apenas 'file_id'\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dir, transform=None, has_labels=True):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.has_labels = has_labels\n",
    "        \n",
    "        # Descobrir classes automaticamente a partir dos dados\n",
    "        if self.has_labels:\n",
    "            self.classes = sorted(self.df['class'].unique())\n",
    "            self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        else:\n",
    "            self.classes = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.df.iloc[idx]['file_id']\n",
    "        img_path = os.path.join(self.img_dir, f'{file_id}.jpg')\n",
    "        \n",
    "        # Carregar imagem\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Aplicar transforma√ß√µes se fornecidas\n",
    "        if self.transform:\n",
    "            pixels = self.transform(image)\n",
    "        else:\n",
    "            pixels = image\n",
    "        \n",
    "        # Retornar dados\n",
    "        if self.has_labels:\n",
    "            label = self.df.iloc[idx]['class']\n",
    "            return {'pixels': pixels, 'label': label, 'img': image, 'file_id': file_id}\n",
    "        else:\n",
    "            # Para test set: sem label, apenas ID e pixels\n",
    "            return {'pixels': pixels, 'file_id': file_id, 'img': image}\n",
    "\n",
    "# ============================================================================\n",
    "# CARREGAR DADOS CUSTOMIZADOS\n",
    "# ============================================================================\n",
    "# CEN√ÅRIO REAL: \n",
    "#   - train.csv tem labels ‚Üí usado para treinar\n",
    "#   - test.csv N√ÉO tem labels ‚Üí usado para submiss√£o Kaggle\n",
    "\n",
    "# Caminho dos dados\n",
    "TRAIN_CSV = '/workspace/data/train.csv'\n",
    "TEST_CSV = '/workspace/data/test.csv'\n",
    "TRAIN_IMG_DIR = '/workspace/data/train'\n",
    "TEST_IMG_DIR = '/workspace/data/test'\n",
    "\n",
    "# Carregar datasets com labels (para treino)\n",
    "full_dataset = CustomImageDataset(\n",
    "    csv_file=TRAIN_CSV, \n",
    "    img_dir=TRAIN_IMG_DIR, \n",
    "    transform=None,  # Aplicaremos transforma√ß√µes depois\n",
    "    has_labels=True\n",
    ")\n",
    "\n",
    "# Dividir em treino e valida√ß√£o (estratificado para manter propor√ß√µes)\n",
    "# IMPORTANTE: Isso garante que train/val tenham mesma distribui√ß√£o de classes\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "trainds, valds = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Carregar test set SEM labels (cen√°rio real de competi√ß√£o Kaggle)\n",
    "testds = CustomImageDataset(\n",
    "    csv_file=TEST_CSV,\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=None,\n",
    "    has_labels=False  # IMPORTANTE: test n√£o tem labels em competi√ß√µes\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(trainds)}\")\n",
    "print(f\"Val samples: {len(valds)}\")\n",
    "print(f\"Test samples: {len(testds)}\")\n",
    "print(f\"Classes identificadas: {full_dataset.classes}\")\n",
    "print(f\"N√∫mero de classes: {len(full_dataset.classes)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Usando Hugging Face datasets:\n",
    "# trainds, testds = load_dataset(\"cifar10\", split=[\"train[:5000]\",\"test[:1000]\"])\n",
    "# splits = trainds.train_test_split(test_size=0.1)\n",
    "# trainds = splits['train']\n",
    "# valds = splits['test']\n",
    "# trainds, valds, testds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CRIAR MAPEAMENTOS DE CLASSES (itos/stoi)\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: Agora classes v√™m automaticamente do dataset customizado\n",
    "# RAZ√ÉO: Em dados reais, o n√∫mero de classes √© din√¢mico\n",
    "# COMPARA√á√ÉO: Antes us√°vamos trainds.features['label'].names (Hugging Face)\n",
    "#            Agora usamos full_dataset.classes (descoberto automaticamente do CSV)\n",
    "\n",
    "# itos = int-to-string: mapeia √≠ndice de classe para nome leg√≠vel\n",
    "#   Exemplo: 0 ‚Üí 'class_0', 1 ‚Üí 'class_1'\n",
    "# stoi = string-to-int: mapeamento inverso\n",
    "#   Exemplo: 'class_0' ‚Üí 0, 'class_1' ‚Üí 1\n",
    "\n",
    "# ESTRUTURA DO DADO: train.csv tem n√∫meros de classe (0, 1, 2, ...)\n",
    "# Criamos nomes leg√≠veis associados\n",
    "\n",
    "class_names = [f'class_{i}' for i in full_dataset.classes]\n",
    "itos = dict(enumerate(class_names))\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "\n",
    "print(\"Mapeamento INT-TO-STRING (itos):\")\n",
    "print(itos)\n",
    "print(\"\\nMapeamento STRING-TO-INT (stoi):\")\n",
    "print(stoi)\n",
    "print(f\"\\nTotal de classes: {len(itos)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Usando Hugging Face naming:\n",
    "# itos = dict((k,v) for k,v in enumerate(trainds.features['label'].names))\n",
    "# stoi = dict((v,k) for k,v in enumerate(trainds.features['label'].names))\n",
    "# itos, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: VISUALIZAR AMOSTRA DO DATASET - ANTES DE TRANSFORMA√á√ïES\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: Agora trabalhamos com dataset customizado que retorna dicts\n",
    "# RAZ√ÉO: Estrutura diferente de Hugging Face datasets\n",
    "# IMPORTANTE: Visualizar ANTES das transforma√ß√µes para ver imagem original\n",
    "\n",
    "# Pegar primeiro sample do dataset de treino (sem transforma√ß√µes ainda)\n",
    "train_dataset_raw = full_dataset  # Dataset bruto, sem transforma√ß√µes\n",
    "\n",
    "index = 0\n",
    "sample = train_dataset_raw[index]\n",
    "\n",
    "print(f\"Estrutura do sample retornado:\")\n",
    "print(f\"  - pixels: tipo {type(sample['pixels'])}\")\n",
    "print(f\"  - label: {sample['label']}\")\n",
    "print(f\"  - file_id: {sample['file_id']}\")\n",
    "print(f\"  - classe interpretada: {class_names[sample['label']]}\")\n",
    "\n",
    "# Exibir imagem\n",
    "img = sample['img']\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Classe: {class_names[sample['label']]} (ID do arquivo: {sample['file_id']})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL:\n",
    "# index = 0\n",
    "# img, lab = trainds[index]['img'], itos[trainds[index]['label']]\n",
    "# print(lab)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ae09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: CARREGAR PROCESSADOR E EXTRAIR PAR√ÇMETROS DE NORMALIZA√á√ÉO\n",
    "# ============================================================================\n",
    "# SEM ALTERA√á√ïES: Este c√≥digo continua o mesmo\n",
    "# RAZ√ÉO: O processador do ViT √© universal e funciona igual\n",
    "# IMPORTANTE: ImageNet normalization √© cr√≠tica para transfer learning\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name) \n",
    "\n",
    "mu, sigma = processor.image_mean, processor.image_std  # Par√¢metros ImageNet\n",
    "size = processor.size\n",
    "\n",
    "print(f\"Modelo: {model_name}\")\n",
    "print(f\"Tamanho de entrada: {size}\")\n",
    "print(f\"M√©dia de normaliza√ß√£o (ImageNet): {mu}\")\n",
    "print(f\"Desvio padr√£o (ImageNet): {sigma}\")\n",
    "print(\"\\nPORQU√ä ImageNet stats? Transfer learning depende que input distribution\")\n",
    "print(\"corresponda exatamente ao que o modelo foi treinado. Usar outros valores\")\n",
    "print(\"causaria catastrophic forgetting dos pesos pr√©-treinados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b09cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: DEFINIR PIPELINE DE TRANSFORMA√á√ïES\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO IMPORTANTE: Usar Compose corretamente para garantir tamanho fixo\n",
    "# PROBLEMA ORIGINAL: Resize sem CenterCrop pode manter aspect ratio\n",
    "# SOLU√á√ÉO: For√ßar tamanho exato 224x224 com CenterCrop\n",
    "\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "norm = Normalize(mean=mu, std=sigma)  # Normalizar para [-1,1]\n",
    "\n",
    "# Pipeline de transforma√ß√£o CORRIGIDO:\n",
    "#   1. Resize(256): Redimensiona mantendo aspect ratio (lado menor = 256)\n",
    "#   2. CenterCrop(224): Recorta centro para 224x224 (GARANTE TAMANHO FIXO!)\n",
    "#   3. ToTensor(): PIL Image ‚Üí Tensor PyTorch\n",
    "#   4. Normalize(): Aplicar mean/std ImageNet\n",
    "_transf = Compose([\n",
    "    Resize(256),              # Redimensiona lado menor para 256 (mant√©m aspect ratio)\n",
    "    CenterCrop(224),          # ‚úÖ NOVO: Garante exatamente 224x224\n",
    "    ToTensor(),\n",
    "    norm\n",
    "]) \n",
    "\n",
    "print(\"Pipeline de transforma√ß√µes (CORRIGIDO):\")\n",
    "print(\"  1. Resize(256) - redimensiona mantendo aspect ratio\")\n",
    "print(\"  2. CenterCrop(224) - ‚úÖ NOVO: garante tamanho uniforme 224x224\")\n",
    "print(\"  3. ToTensor() - converter PIL Image para torch.Tensor\")\n",
    "print(\"  4. Normalize(mu, sigma) - normalizar com ImageNet stats\")\n",
    "print(\"  5. Output: tensor 3x224x224 com valores em [-1, 1]\")\n",
    "print(\"\\nüí° Por qu√™ CenterCrop? Imagens com aspect ratios diferentes precisam\")\n",
    "print(\"   ser recortadas para tamanho uniforme antes de fazer stack em batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99372d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: APLICAR TRANSFORMA√á√ïES AO DATASET\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: CustomImageDataset pode receber transforms no __init__\n",
    "# RAZ√ÉO: Mais eficiente que usar set_transform (que √© m√©todo de HF Dataset)\n",
    "# ESTRAT√âGIA: \n",
    "#   - Criar \"wrapper\" que aplica transforma√ß√µes no __getitem__\n",
    "#   - Train/Val: aplicar transforma√ß√µes\n",
    "#   - Test: aplicar transforma√ß√µes (para que shape fique correto)\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    \"\"\"Wrapper que aplica transforma√ß√µes a um dataset existente\"\"\"\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            sample['pixels'] = self.transform(sample['img'])\n",
    "        else:\n",
    "            sample['pixels'] = sample['img']\n",
    "        return sample\n",
    "\n",
    "# Aplicar transforma√ß√µes aos datasets\n",
    "trainds = TransformDataset(trainds, transform=_transf)\n",
    "valds = TransformDataset(valds, transform=_transf)\n",
    "testds = TransformDataset(testds, transform=_transf)\n",
    "\n",
    "print(\"Transforma√ß√µes aplicadas a:\")\n",
    "print(f\"  - Train set ({len(trainds)} amostras)\")\n",
    "print(f\"  - Val set ({len(valds)} amostras)\")\n",
    "print(f\"  - Test set ({len(testds)} amostras - SEM labels)\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Usando m√©todo set_transform (Hugging Face):\n",
    "# # apply transforms to PIL Image and store it to 'pixels' key\n",
    "# def transf(arg):\n",
    "#     arg['pixels'] = [_transf(image.convert('RGB')) for image in arg['img']]\n",
    "#     return arg\n",
    "# \n",
    "# trainds.set_transform(transf)\n",
    "# valds.set_transform(transf)\n",
    "# testds.set_transform(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: VISUALIZAR IMAGEM AP√ìS TRANSFORMA√á√ïES\n",
    "# ============================================================================\n",
    "# SEM ALTERA√á√ïES SIGNIFICATIVAS: Visualiza√ß√£o funciona igual\n",
    "# NOTA: Denormalizamos para poder visualizar (valores voltam a [0,1])\n",
    "\n",
    "idx = 0\n",
    "sample = trainds[idx]\n",
    "ex = sample['pixels']  # Tensor normalizado\n",
    "\n",
    "# Denormalizar para visualizar (reverter Normalize operation)\n",
    "# F√≥rmula: x_original = (x_normalizado * sigma) + mu\n",
    "ex_denorm = (ex * torch.tensor(sigma).view(3, 1, 1)) + torch.tensor(mu).view(3, 1, 1)\n",
    "ex_denorm = torch.clamp(ex_denorm, 0, 1)  # Manter em [0, 1]\n",
    "\n",
    "# Converter para PIL Image e exibir\n",
    "exi = ToPILImage()(ex_denorm)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(exi)\n",
    "plt.title(f\"Imagem ap√≥s transforma√ß√µes - Classe: {class_names[sample['label']]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Shape do tensor: {ex.shape}\")\n",
    "print(f\"Valor m√≠nimo: {ex.min():.4f}, M√°ximo: {ex.max():.4f}\")\n",
    "print(\"‚úì Imagem normalizada corretamente (valores em [-1, 1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a61505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: CARREGAR MODELO PR√â-TREINADO (SEM ADAPTA√á√ÉO)\n",
    "# ============================================================================\n",
    "# SEM ALTERA√á√ïES: Carregar modelo em sua forma original\n",
    "# RAZ√ÉO: Pr√≥ximo passo ser√° adaptar para o n√∫mero correto de classes\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Modelo original carregado: {model_name}\")\n",
    "print(f\"Camada de classifica√ß√£o original (1000 classes ImageNet):\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a51af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: ADAPTAR MODELO PARA N√öMERO CORRETO DE CLASSES\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: usar len(full_dataset.classes) em vez de hardcoded 10\n",
    "# RAZ√ÉO: Torna c√≥digo gen√©rico para qualquer n√∫mero de classes\n",
    "# IMPORTANTE: ignore_mismatched_sizes=True permite redimensionar classifier layer\n",
    "\n",
    "num_labels = len(full_dataset.classes)\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,  # Din√¢mico baseado nos dados\n",
    "    ignore_mismatched_sizes=True,  # Permite adaptar 1000‚Üínum_labels\n",
    "    id2label=itos,  # Mapeamento √≠ndice ‚Üí nome\n",
    "    label2id=stoi   # Mapeamento nome ‚Üí √≠ndice\n",
    ")\n",
    "\n",
    "print(f\"Modelo adaptado para {num_labels} classes\")\n",
    "print(f\"Camada de classifica√ß√£o adaptada:\")\n",
    "print(model.classifier)\n",
    "print(f\"\\nMapeamentos configurados:\")\n",
    "print(f\"  id2label: {itos}\")\n",
    "print(f\"  label2id: {stoi}\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Hardcoded para CIFAR-10 (10 classes):\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     model_name, \n",
    "#     num_labels=10,  # Hardcoded\n",
    "#     ignore_mismatched_sizes=True, \n",
    "#     id2label=itos, \n",
    "#     label2id=stoi\n",
    "# )\n",
    "# print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: CONFIGURAR ARGUMENTOS DE TREINAMENTO\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: Otimizar batch_size e epochs para treinamento mais r√°pido\n",
    "# RAZ√ÉO: Batch size original (10) era muito pequeno\n",
    "# ESTRAT√âGIA: Se GPU dispon√≠vel, usar batch_size maior; sen√£o, manter pequeno\n",
    "\n",
    "# Detectar se tem GPU para otimizar\n",
    "has_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Otimizar batch size baseado em hardware\n",
    "if has_gpu:\n",
    "    train_batch_size = 32  # GPU pode lidar com batches maiores\n",
    "    eval_batch_size = 64   # Avalia√ß√£o pode ser ainda maior\n",
    "    num_epochs = 3\n",
    "    print(\"üöÄ GPU detectada! Usando batch sizes otimizados\")\n",
    "else:\n",
    "    train_batch_size = 10  # CPU precisa de batches menores\n",
    "    eval_batch_size = 10\n",
    "    num_epochs = 3\n",
    "    print(\"‚ö†Ô∏è  GPU N√ÉO detectada. Usando batch sizes para CPU (treinamento ser√° lento)\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-custom-dataset\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_size,  # ‚úÖ OTIMIZADO\n",
    "    per_device_eval_batch_size=eval_batch_size,    # ‚úÖ OTIMIZADO\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.04,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='logs',\n",
    "    logging_steps=50,  # Log a cada 50 steps (para ver progresso)\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Configura√ß√£o de treinamento:\")\n",
    "print(f\"  - Output dir: {args.output_dir}\")\n",
    "print(f\"  - Learning rate: {args.learning_rate}\")\n",
    "print(f\"  - Batch size TREINO: {args.per_device_train_batch_size}\")\n",
    "print(f\"  - Batch size VALIDA√á√ÉO: {args.per_device_eval_batch_size}\")\n",
    "print(f\"  - Epochs: {args.num_train_epochs}\")\n",
    "print(f\"  - M√©trica para melhor modelo: {args.metric_for_best_model}\")\n",
    "print(f\"  - GPU: {'‚úÖ Sim' if has_gpu else '‚ùå N√£o'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Hardcoded, batch size muito pequeno:\n",
    "# args = TrainingArguments(\n",
    "#     f\"test-cifar-10\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     eval_strategy =\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=10,  # ‚ùå Muito pequeno\n",
    "#     per_device_eval_batch_size=4,    # ‚ùå Muito pequeno\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.04,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"accuracy\",\n",
    "#     logging_dir='logs',\n",
    "#     remove_unused_columns=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ee70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: DEFINIR FUN√á√ïES DE COLATE E M√âTRICAS\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO: collate_fn agora lida com dataset customizado\n",
    "# RAZ√ÉO: CustomImageDataset retorna dicts com estrutura ligeiramente diferente\n",
    "# IMPORTANTE: Manter file_id nos batches para rastrear predi√ß√µes no test set\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function customizado que transforma lista de examples em batch\n",
    "    \n",
    "    RAZ√ÉO: Trainer espera que pixel_values e labels sejam stacked em dimens√£o batch\n",
    "    \n",
    "    INPUT: lista de dicts com keys ['pixels', 'label', 'file_id', 'img']\n",
    "    OUTPUT: dict com keys ['pixel_values', 'labels', 'file_id']\n",
    "    \"\"\"\n",
    "    # Stack de tensores de pixels (transformando lista em batch)\n",
    "    pixels = torch.stack([example[\"pixels\"] for example in examples])\n",
    "    \n",
    "    # Converter labels para tensor\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    \n",
    "    # Manter file_ids para rastreamento (importante para test set)\n",
    "    file_ids = [example[\"file_id\"] for example in examples]\n",
    "    \n",
    "    # Retornar no formato esperado pelo Trainer\n",
    "    return {\n",
    "        \"pixel_values\": pixels,  # IMPORTANTE: nome correto para ViT (n√£o 'pixels')\n",
    "        \"labels\": labels,\n",
    "        \"file_id\": file_ids  # Adicional: √∫til para submiss√µes Kaggle\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computar m√©tricas de avalia√ß√£o\n",
    "    \n",
    "    RAZ√ÉO: Trainer executa isso ap√≥s cada epoch para monitorar progresso\n",
    "    \n",
    "    INPUT: EvalPrediction com predictions (logits brutos) e labels verdadeiros\n",
    "    OUTPUT: dict com m√©tricas (accuracy, etc)\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)  # Converter logits ‚Üí classes\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n",
    "\n",
    "print(\"‚úì Fun√ß√µes collate_fn e compute_metrics definidas\")\n",
    "print(\"\\ncollate_fn:\")\n",
    "print(\"  - Transforma lista de samples em batch com shapes corretos\")\n",
    "print(\"  - Renomeia 'pixels' ‚Üí 'pixel_values' (obrigat√≥rio para ViT)\")\n",
    "print(\"  - Manter file_ids para rastreamento\")\n",
    "print(\"\\ncompute_metrics:\")\n",
    "print(\"  - Calcula accuracy durante valida√ß√£o\")\n",
    "print(\"  - Permite monitorar progresso do treinamento\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Sem file_id:\n",
    "# def collate_fn(examples):\n",
    "#     pixels = torch.stack([example[\"pixels\"] for example in examples])\n",
    "#     labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "#     return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "# \n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return dict(accuracy=accuracy_score(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08efcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: CRIAR TRAINER\n",
    "# ============================================================================\n",
    "# SEM ALTERA√á√ïES SIGNIFICATIVAS: Trainer funciona igual com datasets customizados\n",
    "# IMPORTANTE: Trainer autom√°ticamente usa collate_fn que definimos\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args, \n",
    "    train_dataset=trainds,\n",
    "    eval_dataset=valds,\n",
    "    data_collator=collate_fn,  # Nossa fun√ß√£o customizada\n",
    "    compute_metrics=compute_metrics,  # Nossa fun√ß√£o de m√©tricas\n",
    "    tokenizer=processor,  # Processador do ViT\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer criado com sucesso\")\n",
    "print(f\"  - Modelo: {model.config.architectures[0]}\")\n",
    "print(f\"  - Classes: {num_labels}\")\n",
    "print(f\"  - Train batches: {len(trainds) // args.per_device_train_batch_size}\")\n",
    "print(f\"  - Val batches: {len(valds) // args.per_device_eval_batch_size}\")\n",
    "print(f\"  - Epochs: {args.num_train_epochs}\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL:\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args, \n",
    "#     train_dataset=trainds,\n",
    "#     eval_dataset=valds,\n",
    "#     data_collator=collate_fn,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=processor,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7132bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14B: DIAGN√ìSTICO E VALIDA√á√ÉO DE DADOS (NOVO)\n",
    "# ============================================================================\n",
    "# RAZ√ÉO: Detectar problemas nos dados ANTES de treinar\n",
    "# IMPORTANTE: Verificar:\n",
    "#   1. Imagens faltando no disco\n",
    "#   2. Tamanhos inconsistentes (causa erro de stack)\n",
    "#   3. GPU dispon√≠vel (determina velocidade)\n",
    "#   4. Estimar tempo de treinamento\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç DIAGN√ìSTICO DE DADOS E HARDWARE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Verificar GPU\n",
    "print(\"\\n1Ô∏è‚É£  VERIFICAR GPU:\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"  Dispositivo dispon√≠vel: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  GPU N√ÉO DISPON√çVEL! Treinamento ser√° MUITO lento (CPU)\")\n",
    "\n",
    "# 2. Verificar integridade das imagens e tamanhos\n",
    "print(\"\\n2Ô∏è‚É£  VERIFICAR INTEGRIDADE DAS IMAGENS:\")\n",
    "\n",
    "# Verificar train\n",
    "missing_train = 0\n",
    "size_issues = 0\n",
    "train_sizes = {}\n",
    "\n",
    "print(\"  Escaneando train dataset...\")\n",
    "for i in range(min(100, len(full_dataset))):  # Verificar primeiras 100\n",
    "    try:\n",
    "        sample = full_dataset[i]\n",
    "        file_id = sample['file_id']\n",
    "        img = sample['img']\n",
    "        size = img.size  # (width, height)\n",
    "        \n",
    "        if size not in train_sizes:\n",
    "            train_sizes[size] = 0\n",
    "        train_sizes[size] += 1\n",
    "    except Exception as e:\n",
    "        missing_train += 1\n",
    "        if missing_train <= 5:  # Mostrar primeiros 5 erros\n",
    "            print(f\"    ‚ùå Erro no √≠ndice {i}: {e}\")\n",
    "\n",
    "print(f\"  ‚úì Train: {len(full_dataset)} amostras verificadas\")\n",
    "if missing_train > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  {missing_train} imagens com problema\")\n",
    "print(f\"  Distribui√ß√£o de tamanhos de imagem (primeiras 100):\")\n",
    "for size, count in sorted(train_sizes.items(), key=lambda x: -x[1])[:5]:\n",
    "    print(f\"    - {size[0]}x{size[1]}: {count} imagens\")\n",
    "\n",
    "# 3. Testar batch com diferentes tamanhos\n",
    "print(\"\\n3Ô∏è‚É£  TESTAR CRIA√á√ÉO DE BATCH:\")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    # Criar mini batch para testar\n",
    "    test_loader = DataLoader(\n",
    "        TransformDataset(torch.utils.data.Subset(trainds.dataset, list(range(min(4, len(trainds.dataset))))), \n",
    "                        transform=_transf),\n",
    "        batch_size=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    batch = next(iter(test_loader))\n",
    "    print(f\"  ‚úì Batch criado com sucesso!\")\n",
    "    print(f\"    - pixel_values shape: {batch['pixel_values'].shape}\")\n",
    "    print(f\"    - labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"    - file_ids: {batch['file_id']}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå ERRO ao criar batch: {e}\")\n",
    "    print(\"  Este √© o mesmo erro que vai ocorrer no treinamento!\")\n",
    "\n",
    "# 4. Estimar tempo de treinamento\n",
    "print(\"\\n4Ô∏è‚É£  ESTIMATIVA DE TEMPO DE TREINAMENTO:\")\n",
    "total_samples = len(trainds)\n",
    "batch_size = args.per_device_train_batch_size\n",
    "num_epochs = args.num_train_epochs\n",
    "total_batches = (total_samples // batch_size) * num_epochs\n",
    "\n",
    "print(f\"  Total de samples: {total_samples}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Total de batches: {total_batches}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    time_per_batch_sec = 0.5  # Estimativa com GPU: 0.5s por batch\n",
    "    estimated_hours = (total_batches * time_per_batch_sec) / 3600\n",
    "    print(f\"\\n  ‚è±Ô∏è  ESTIMADO COM GPU: {estimated_hours:.1f} horas\")\n",
    "else:\n",
    "    time_per_batch_sec = 5.0  # CPU √© ~10x mais lento\n",
    "    estimated_hours = (total_batches * time_per_batch_sec) / 3600\n",
    "    print(f\"\\n  ‚è±Ô∏è  ESTIMADO COM CPU: {estimated_hours:.1f} horas (‚ö†Ô∏è  MUITO LENTO!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15.5: OTIMIZA√á√ÉO ANTES DO TREINAMENTO (NOVO)\n",
    "# ============================================================================\n",
    "# RAZ√ÉO: 4+ horas √© inaceit√°vel. Precisamos acelerar.\n",
    "# ESTRAT√âGIA: \n",
    "#   1. For√ßar modelo para GPU\n",
    "#   2. Usar mixed precision (fp16) se GPU dispon√≠vel\n",
    "#   3. Op√ß√£o de reduzir dataset para teste r√°pido\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö° OTIMIZA√á√ÉO PR√â-TREINAMENTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For√ßar modelo para GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n‚úì Modelo movido para: {device}\")\n",
    "\n",
    "# Configurar mixed precision se GPU dispon√≠vel (reduz mem√≥ria, aumenta velocidade)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚úì GPU detectada - ativando otimiza√ß√µes:\")\n",
    "    \n",
    "    # Mostrar info de GPU\n",
    "    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Adicionar mixed precision aos args\n",
    "    args.fp16 = True  # Usar float16 (mais r√°pido, menos mem√≥ria)\n",
    "    print(f\"  - Mixed precision: ‚úÖ ativado (fp16)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  CPU detectada - treinamento ser√° MUITO lento!\")\n",
    "    print(\"   Considere usar GPU (Colab, cloud, etc)\")\n",
    "\n",
    "# Op√ß√£o 1: Fazer teste r√°pido com subset\n",
    "USE_SUBSET = False  # Mude para True se quiser teste r√°pido\n",
    "if USE_SUBSET:\n",
    "    print(\"\\nüß™ MODO TESTE (subset 10%)\")\n",
    "    # Usar apenas 10% dos dados para teste r√°pido\n",
    "    subset_size = max(10, len(trainds) // 10)\n",
    "    val_subset_size = max(5, len(valds) // 10)\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "    trainds = Subset(trainds, list(range(subset_size)))\n",
    "    valds = Subset(valds, list(range(val_subset_size)))\n",
    "    \n",
    "    print(f\"  Train: {len(trainds)} ‚Üí {subset_size} samples\")\n",
    "    print(f\"  Val: {len(valds)} ‚Üí {val_subset_size} samples\")\n",
    "    \n",
    "    # Recalcular epochs para teste (usar menos)\n",
    "    args.num_train_epochs = 1\n",
    "    print(f\"  Epochs: 3 ‚Üí 1 (teste r√°pido)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚è±Ô∏è  Tempo estimado AGORA: \", end=\"\")\n",
    "if torch.cuda.is_available() and not USE_SUBSET:\n",
    "    print(\"~30-45 min (GPU otimizada)\")\n",
    "elif USE_SUBSET:\n",
    "    print(\"~2-3 min (subset teste)\")\n",
    "else:\n",
    "    print(\"~3-5 horas (CPU - pode interromper com Ctrl+C)\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: TREINAR MODELO\n",
    "# ============================================================================\n",
    "# SEM ALTERA√á√ïES: Treinamento funciona igual\n",
    "# COMPORTAMENTO ESPERADO:\n",
    "#   1. Salva checkpoint a cada epoch\n",
    "#   2. Valida a cada epoch\n",
    "#   3. Mant√©m melhor modelo baseado em accuracy de valida√ß√£o\n",
    "#   4. Exibe progresso em tempo real\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d07ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 16: FAZER PREDI√á√ïES NO TEST SET (SEM LABELS)\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO PRINCIPAL: Test set N√ÉO tem labels (cen√°rio Kaggle real)\n",
    "# RAZ√ÉO: Em competi√ß√µes, voc√™ s√≥ faz predict sem comparar com verdade\n",
    "# IMPORTANTE: Rastrear file_ids para submiss√£o correta\n",
    "\n",
    "# Fazer predi√ß√µes no test set\n",
    "outputs = trainer.predict(testds)\n",
    "\n",
    "print(\"Predi√ß√µes completadas!\")\n",
    "print(f\"Shape das predi√ß√µes (logits): {outputs.predictions.shape}\")\n",
    "print(f\"  - Dimens√£o 0 (samples): {outputs.predictions.shape[0]}\")\n",
    "print(f\"  - Dimens√£o 1 (classes): {outputs.predictions.shape[1]}\")\n",
    "\n",
    "# Converter logits em classes preditas\n",
    "predicted_classes = np.argmax(outputs.predictions, axis=1)\n",
    "print(f\"\\nPredi√ß√µes (classe √≠ndice): {predicted_classes[:10]}\")  # Primeiras 10\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - funcionava mas sem rastreamento de file_ids:\n",
    "# outputs = trainer.predict(testds)\n",
    "# print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 17: GERAR ARQUIVO DE SUBMISS√ÉO KAGGLE\n",
    "# ============================================================================\n",
    "# NOVO CELL: N√£o existia no c√≥digo antigo (agora necess√°rio para workflow real)\n",
    "# RAZ√ÉO: Competi√ß√µes Kaggle exigem arquivo no formato espec√≠fico\n",
    "# FORMATO: file_id, class (predito)\n",
    "\n",
    "# Extrair file_ids do test set (mantendo ordem)\n",
    "test_file_ids = []\n",
    "for i in range(len(testds.dataset)):\n",
    "    sample = testds.dataset[i]\n",
    "    test_file_ids.append(sample['file_id'])\n",
    "\n",
    "# Criar DataFrame de submiss√£o\n",
    "submission_df = pd.DataFrame({\n",
    "    'file_id': test_file_ids,\n",
    "    'class': predicted_classes\n",
    "})\n",
    "\n",
    "print(\"DataFrame de submiss√£o:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nTotal de predi√ß√µes: {len(submission_df)}\")\n",
    "print(f\"Classes preditas - distribui√ß√£o:\")\n",
    "print(submission_df['class'].value_counts().sort_index())\n",
    "\n",
    "# Salvar arquivo de submiss√£o\n",
    "submission_path = '/workspace/submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úì Arquivo de submiss√£o salvo em: {submission_path}\")\n",
    "\n",
    "# Verificar arquivo criado\n",
    "print(\"\\nPrimeiras linhas do arquivo de submiss√£o:\")\n",
    "print(pd.read_csv(submission_path).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 18: AN√ÅLISE E CONFUSION MATRIX (VALIDATION SET APENAS)\n",
    "# ============================================================================\n",
    "# ALTERA√á√ÉO IMPORTANTE: Analisamos apenas VAL set (que tem labels)\n",
    "# RAZ√ÉO: Test set n√£o tem labels verdadeiros (cen√°rio Kaggle)\n",
    "# M√âTRICA: Confusion matrix mostra performance por classe\n",
    "# INTERPRETA√á√ÉO: Diagonal alta = bom, off-diagonal = erros entre pares de classes\n",
    "\n",
    "# Fazer predi√ß√µes no validation set (que tem labels verdadeiros)\n",
    "val_outputs = trainer.predict(valds)\n",
    "\n",
    "print(\"An√°lise de Valida√ß√£o:\")\n",
    "print(f\"Accuracy no validation set: {val_outputs.metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "# Extrair predictions e labels verdadeiros\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(valds.dataset)):\n",
    "    sample = valds.dataset[i]\n",
    "    y_true.append(sample['label'])\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.argmax(val_outputs.predictions, axis=1)\n",
    "\n",
    "# Criar confusion matrix\n",
    "labels_list = list(itos.values())\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(num_labels))\n",
    "\n",
    "# Exibir confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_list)\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix - Validation Set (Accuracy: {val_outputs.metrics.get('accuracy', 'N/A'):.4f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise por classe\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AN√ÅLISE POR CLASSE (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=range(num_labels), zero_division=0\n",
    ")\n",
    "\n",
    "for i in range(num_labels):\n",
    "    print(f\"\\nClasse {i} ({labels_list[i]}):\")\n",
    "    print(f\"  Precision: {precision[i]:.4f} (% acertos quando prediz classe i)\")\n",
    "    print(f\"  Recall:    {recall[i]:.4f} (% classe i detectados)\")\n",
    "    print(f\"  F1-Score:  {f1[i]:.4f}\")\n",
    "    print(f\"  Suporte:   {support[i]} (samples)\")\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO ANTIGO (COMENTADO) - Para refer√™ncia\n",
    "# ============================================================================\n",
    "# # ORIGINAL - Testava no test set (que tinha labels em CIFAR-10):\n",
    "# outputs = trainer.predict(testds)\n",
    "# print(outputs.metrics)\n",
    "# \n",
    "# itos[np.argmax(outputs.predictions[0])], itos[outputs.label_ids[0]]\n",
    "# \n",
    "# y_true = outputs.label_ids\n",
    "# y_pred = outputs.predictions.argmax(1)\n",
    "# \n",
    "# labels = trainds.features['label'].names\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "# disp.plot(xticks_rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
